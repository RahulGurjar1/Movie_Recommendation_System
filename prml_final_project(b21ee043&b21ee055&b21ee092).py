# -*- coding: utf-8 -*-
"""PRML_FINAL_PROJECT(B21EE043&B21EE055&B21EE092).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ffEfHajG4cW_oWiJAV3OKH5oxfr33Pxq

# Initial Code
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

df_movies=pd.read_csv('/kaggle/input/movie-recommendation-system/movies.csv', encoding='ISO-8859-1')
df_movies
df_movies2=df_movies

df_movie_Id=df_movies.iloc[:,:2];
df_movie_Id

df_ratings=pd.read_csv('/kaggle/input/movie-recommendation-system/ratings.csv', encoding='ISO-8859-1')
df_ratings
df_ratings2=df_ratings

df_links=pd.read_csv('/kaggle/input/movie-recommendation-system/links.csv', encoding='ISO-8859-1')
df_links

df_tags=pd.read_csv('/kaggle/input/movie-recommendation-system/tags.csv', encoding='ISO-8859-1')
df_tags

"""# **Analysis of tags dataset**"""

df_tags.nunique()

df_tags['Datetime'] = pd.to_datetime(df_tags['timestamp'],unit='s')
df_tags

df_tags['year'] = pd.DatetimeIndex(df_tags['Datetime']).year
df_tags['month'] = pd.DatetimeIndex(df_tags['Datetime']).month
df_tags.drop(['Datetime','timestamp'],axis=1,inplace=True)
df_tags

import plotly.express as px
fig = px.pie(df_tags, values='month', names='month', title='Monthly Distribution')
fig.show()
fig2 = px.pie(df_tags, values='year', names='year', title='Yearly Distribution')
fig2.show()
# fig3 = px.pie(df_tags, values='tag', names='tag', title='Distribution by tag')
# fig3.show()

from sklearn.preprocessing import LabelEncoder
column_names=['tag','year'];
for col in column_names:
  le=LabelEncoder();
  values = le.fit_transform(df_tags[col]);
  df_tags[col]=values[:];
df_tags

print("NULL values in the dataset:\n",df_tags.isnull().sum());

"""# **Analysis of ratings dataset**"""

df_ratings

df_ratings.nunique()

df_ratings.describe()

df_ratings.isnull().sum()

rating_unique_values = df_ratings["rating"].unique()
print('Different Unique values in ratings column:',rating_unique_values)

ratings_counts = []
for value in rating_unique_values:
    count = df_ratings["rating"].value_counts()[value]
    ratings_counts.append(count)
rating_mapping_dataframe=pd.DataFrame({'Rating value':rating_unique_values,'Total Count':ratings_counts})
rating_mapping_dataframe.T

df_ratings['Datetime'] = pd.to_datetime(df_ratings['timestamp'],unit='s')
df_ratings

df_ratings['year'] = pd.DatetimeIndex(df_ratings['Datetime']).year
df_ratings['month'] = pd.DatetimeIndex(df_ratings['Datetime']).month
df_ratings.drop(['Datetime','timestamp'],axis=1,inplace=True)
le=LabelEncoder();
values = le.fit_transform(df_ratings['year']);
df_ratings['year']=values[:];
df_ratings
df_user_ratings=df_ratings;

df_ratings.nunique()

#Creating Movie Based Rating Dataset
df_ratingsmb=df_ratings[['movieId','rating','year']];
r_unique=df_ratings['movieId'].unique();
df_count=df_ratingsmb['movieId'].value_counts()


#*********************************************
# for u_value in r_unique:
#     df_reduced=df_ratingsmb[df_ratingsmb['movieId']==u_value];
#     mean_val=df_reduced['rating'].mean();
#     indexes=df_reduced.index;
#     for ind in indexes:
#         df_ratings.iloc[ind,1]
#     print(indexes);
#     break;
# print(df_ratingsmb.nunique());
#********************************************


df_ratingsmb2=df_ratingsmb.groupby('movieId')['rating'].mean()
df_ratingsmb2=df_ratingsmb2.reset_index();
rows_ds,cols_ds=df_ratingsmb2.shape;


df_count=df_ratingsmb.groupby('movieId').agg(['count'])
df_count=df_count.drop(columns=[(  'year', 'count')])
print(df_count.columns)
# df_count.columns.values[:][:] =["movieId", "rating" ]
df_count.rename(columns = {('rating', 'count'):'Rating'}, inplace = True)
df_count

df_updated_mb=pd.merge(df_count,df_ratingsmb2, on="movieId");
df_updated_mb

"""# **Analysis of Links dataset**"""

df_links

df_links.nunique()

df_links.isnull().sum()

df_links.dropna()
print(df_links.shape);

"""# Analysis of Movies Dataset"""

df_movies

"""## finding the rows with duplicate title"""

duplicate_movies=df_movies[df_movies.duplicated(subset='title',keep=False)]
duplicate_movies

"""## finding the count of every genre and plotting it."""

## finding the unique values in the dataset. 
print(df_movies.nunique())
ct=0;
ls={};
## traverising each genre. 
for i in range(df_movies.shape[0]):
    if(df_movies['genres'][i]=='(no genres listed)'):
        df_movies['genres'][i]='NO GENRE';
    else:
## spliting the genres. 
        ss=df_movies['genres'][i];
        n=len(ss);
        j=0
        word="";
        while(j<n):
            if(ss[j]=='|'):
                j=j+1;
                if(word not in ls):
                    ls[word]=1;
                else:
                    ls[word]=ls[word]+1;
                word="";
            else:
                word=word+ss[j];
                j=j+1;                
        if(word not in ls):
            ls[word]=1;
        else:
            ls[word]=ls[word]+1;
ls1=[];
ls2=[];
for i in ls.keys():
    ls1.append(i);
    ls2.append(ls[i]);
## plotting the figure for genres vs the movies count.
plt_1 = plt.figure(figsize=(40, 20))
plt.bar(ls1,ls2);

"""## Doing oversampling of the dataset due to the genre problem"""

## this function do the oversampling like duplicate the rows. 
df_movies2=(df_movies.set_index(['movieId' , 'title']).apply(lambda x: x.str.split('|').explode()).reset_index())
df_movies2

"""## Extracting all type of genres. """

ct=0;
ls=[];
## finding the unique of genrs. so split the string and get the uniques entries in all of the list. 
for i in range(df_movies.shape[0]):
        ss=df_movies['genres'][i];
        n=len(ss);
        j=0
        word="";
## traversing each line of string
        while(j<n):
            if(ss[j]=='|'):
                j=j+1;
                if(word not in ls):
                    ls.append(word);
                word="";
            else:
                word=word+ss[j];
                j=j+1;                
        if(word not in ls):
            ls.append(word);
print(ls);

"""## assigning a new column of genre in the dataframe and filling its values. """

## now create a new column named genres and then fill those columns. 
for i in ls:
    newcol=np.zeros([df_movies.shape[0],1]);
    df_movies[i]=newcol;
for i in range(df_movies.shape[0]):
        ss=df_movies['genres'][i];
        list_of=ss.split('|');
        for j in list_of:
            df_movies[j][i]=1;

"""## droping the genre column that have string genres. """

## droping the genres (string) column. 
df_movies=df_movies.drop('genres', axis='columns')

"""## handling the entries that are having same title but different genres.  like in this we are checking if a same title having the different genres. that is a fault in dataset. so we are combining there genres # if a movie A having x,y genre and movie B having y,z genre and we will assign A and B x,y,z genres. """

mylist=df_movies['title'].unique();
# print(df_movies.nunique())
listt=df_movies.columns;
# print(listt);
lisst=[];
for k in listt:
    lisst.append(k);
lisst.remove('title');
lisst.remove('movieId');
## finding if any row has two title or not. if yes then equal both of the genres with union of both 
for j in mylist:
    newwd=df_movies[df_movies['title']==j] 
    listtf=newwd.index;
    if(newwd.shape[0]==2):
        for i in lisst:
            if(df_movies[i][listtf[0]]!=df_movies[i][listtf[1]]):
                df_movies[i][listtf[0]]=1;
                df_movies[i][listtf[1]]=1;
# print(df_movies);

newcol=np.zeros([df_movies.shape[0],1]);
df_movies['released']=newcol;

for i in range(32918):
    if(i!=10603 and i!=15674 and i!=17376 and i!=19912 and i!=22471 and i!=22782 and i!=27991 and i!=28061 and i!=32394 and i!=32752
      and i!=22792 and i!=23771 and i!=23992 and  i!=24479 and i!=24608 and i!=27778 and i!=28005 and i!=28193 and i!=32717 and i!=32764
      and i!=27007 and i!=27024 and i!=27090 and i!=27249 and i!=27321 and i!=27358 and i!=27741 and i!=30166 and i!=31627 and i!=31946
      and i!=28884 and i!=28984 and i!=29298 and i!=29508 and i!=29510 and i!=29557  and i!=29740 and i!=29941 and i!=31561 and i!=31806
      and i!=30281 and i!=30285 and i!=31040 and i!=31103 and i!=31299 and i!=31317 and i!=31454 and i!=31512 and i!=31545 and i!=31598
      and i!=32767 and i!=32869 and i!=32879 and i!=32882 and i!=32909):        
        val=df_movies['title'][i];
        j=len(val)-1;
        year="";
        while(val[j]!=')' and (j>=0)):
            j=j-1;
        j=j-1;
        while((val[j]!='(') and (j>=0)):
            year=val[j]+year;
            j=j-1;
#         print(year);
#         yearr=float(year)
        df_movies['released'][i]=year;
    else:
       df_movies['released'][i]=2000;
for i in range(32919,df_movies.shape[0]):
    if(i!=34208):        
        val=df_movies['title'][i];
        j=len(val)-1;
        year="";
        while(val[j]!=')' and (j>=0)):
            j=j-1;
        j=j-1;
        while((val[j]!='(') and (j>=0)):
            year=val[j]+year;
            j=j-1;
#         yearr=float(year)
        df_movies['released'][i]=year;
    else:
       df_movies['released'][i]=2000;
df_movies

"""#Some Analysis using plots """

unique_values = df_ratings["rating"].unique()
print(unique_values)

counts = []

for value in unique_values:
    count = df_ratings["rating"].value_counts()[value]
    counts.append(count)
print(counts)

sns.barplot(x=unique_values,y=counts)

unique_df = pd.DataFrame({"value": unique_values, "count": counts})

unique_df

# df_ratings.plot.pie(y='rating',figsize=(10,10))

report_retail = create_report(rating)
report_retail.show()

report_retail = create_report(df_movies)
report_retail.show()

"""# **Combined Datasets**"""

movies_merged=pd.merge(df_updated_mb,df_movies, on="movieId");
movies_merged
movies_merged=movies_merged.drop(columns=['title'])
movies_merged['released'] = pd.to_numeric(movies_merged['released']);
movies_merged['released']=movies_merged['released'].fillna(2000)
movies_merged

df_tags2=df_tags.drop(['year','month'],axis=1)
user_merged_temp=pd.merge(df_tags2,df_ratings, on=["userId",'movieId']);
user_merged=pd.merge(user_merged_temp,df_movies, on=['movieId']);
user_merged['released'] = pd.to_numeric(user_merged['released']);
user_merged2=user_merged.drop(['title'], axis='columns')
user_merged2['released']=user_merged2['released'].fillna(user_merged2['released'].mode())
user_merged

"""## 1. First we will make a general recommender like if a new user login is happenning. then we will recommend him the best movies based on the rating and releasing date of that movie. """

def general_recommender(dataset,no_of_movies):
    ## sorting the movies based on their rating and streaming year and popularity. 
    new_dataset=dataset.sort_values(by = [('rating', 'count'),'rating','released'],ascending = False);
    listt=new_dataset.index;   
    
     ## now just extract the movie title with the movie id.
    final_dataset2=df_movies[df_movies['movieId']==new_dataset['movieId'][listt[0]]]
    for i in range(1,no_of_movies):
        dataf=df_movies[df_movies['movieId']==new_dataset['movieId'][listt[i]]]
        final_dataset2=pd.concat([dataf,final_dataset2]);
    return final_dataset2['title'].head(no_of_movies);

general_recommender(movies_merged,100)

"""### 2.Recommender based on generes. like we are recommending movies if any user is giving us his favourite genres. """

def genres_based_recommender(dataset,no_of_movies,genres):
    ## here we are just getting the top movies with 1st genre
    final_dataset=dataset[dataset[genres[0]]==1];
    final_dataset=final_dataset.sort_values(by = [('rating', 'count'),'rating','released'],ascending = False)
    final_datset=final_dataset.head(100);
    
    ## now getting the top 100 movies for every genres and the sort them and merge them 
    for i in range(1,len(genres)):
        new_dataset=dataset[dataset[genres[i]]==1];
        new_dataset=new_dataset.sort_values(by =[('rating', 'count'),'rating','released'],ascending = False)
        new_dataset=new_dataset.head(100);
        final_dataset=pd.merge(final_dataset,new_dataset,on =['movieId'])
        
    ## again sort the final dataset and got best movies based on these gneres. 
    final_dataset=final_dataset.sort_values(by =[('rating', 'count'),'rating','released'],ascending = False)

    ## having the index of every movie id 
    listt=final_dataset.index;    
    
     ## now just extract the movie title with the movie id.
    final_dataset2=df_movies[df_movies['movieId']==final_dataset['movieId'][listt[0]]]
    for i in range(1,no_of_movies):
        dataf=df_movies[df_movies['movieId']==final_dataset['movieId'][listt[i]]]
        final_dataset2=pd.concat([dataf,final_dataset2]);
    return final_dataset2['title'].head(no_of_movies);

genres_based_recommender(movies_merged,7,['Adventure','Animation','Children'])

"""## 3. Now we will make a function that will recommend us movies if we give a movie name to the function. this function will recommend movies that are highly correlated to the input movie"""

def movie_based(dataset,no_of_movies,moviename):       
    newdataset=dataset.pivot_table(index=['userId'],columns=['title'],values=['rating']);
    newd=newdataset[('rating',moviename)];    
    correlation=newdataset.corrwith(newd).sort_values(ascending=False);
    listt=correlation.head(no_of_movies);
    print(listt);
movie_based(user_merged,5,'Jumanji (1995)');

"""# 4. using clustering algorithm to recommend movies basically we are clustering all the movies based on there generes and year of released and the ratings. than if we have a used that is asking for a movie similiar to a given movie we will recommend him the movies belong to that cluster sorted by there ratings and there rating counts.

## elbow mehtod
"""

from sklearn.decomposition import PCA
from numpy.linalg import eig
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
print(user_merged2)

def find_best_clusters(data):
    list_of_cluster=[];
    for i in range(6,500,50):
        list_of_cluster.append(i);
    store_out=[]; ## in this list appending the values 
    score_mat=[];
    for i in list_of_cluster:
        model=KMeans(n_clusters=i); ## now traing and fit the modle 
        model=model.fit(data);
        moo=model.predict(data);
        acci=model.inertia_;   ## predict the inerial value. 
        store_out.append(acci);# print(slis);  
    plt.plot(list_of_cluster,store_out);
    plt.grid(); ## plot it in a grid 
    plt.show();

find_best_clusters(movies_merged)

"""## recommending movies by using Kmeans algorithm as a subpart of our problem

# Kmeans
"""

from sklearn.preprocessing import MinMaxScaler
import plotly.graph_objects as go
## as we got the best no. of clusters now we will train our final model and will get the desired results. 
def find_based_on_clustering(data,no_of_cluster,moviename,no_of_movies,fl):
    
    ## finding the movieId the the name of movie;
    model_trained=KMeans(n_clusters=no_of_cluster); ## now traing and fit the modle 
    
    ## training the model 
    scale=MinMaxScaler(feature_range=(0, 5));
    data2=data.copy();
    
    data22=data2[('rating', 'count')]
    data22=pd.DataFrame(data22, columns=[('rating', 'count')]);
    data2=data2.drop(columns=[('rating', 'count')])
    data22=scale.fit_transform(data22.to_numpy())
#     data2[]
    data22=pd.DataFrame(data22, columns=[('rating', 'count')]);
    data22=pd.concat([data22,data2],1)
   
    model_trained=model_trained.fit(data22);
    classes=model_trained.predict(data22);
    
    ## adding a now column of class in which we are storing that a given movie belonging to which cluster. 
    movies_merged2=data22.copy();
    movies_merged2['classes']=(np.array(classes)).transpose()
        
    given_frame=df_movies[df_movies['title']==moviename]
    given_id=given_frame['movieId'][given_frame.index[0]];

    the_frame=movies_merged2[movies_merged2['movieId']==given_id];
    the_class=the_frame['classes'][the_frame.index[0]];
    
    listt = model_trained.cluster_centers_   
    listt=pd.DataFrame(listt);  
    
    the_selected_movie_set=movies_merged2[movies_merged2['classes']==the_class];
    the_selcted_movie_set=the_selected_movie_set.sort_values(by = ['rating','released'],ascending = False)
    from sklearn.preprocessing import LabelEncoder
    le=LabelEncoder();
    movies_merged2['released'] = le.fit_transform(movies_merged2['released']);
    plt.scatter(movies_merged2.iloc[:,1],movies_merged2.iloc[:,2],c=classes)
#     plt.scatter(listt.iloc[:,0],listt.iloc[:,1],color='red');
    plt.show();
    fig = go.Figure(data=[
        go.Scatter3d(
            x=movies_merged2.iloc[:,1].to_numpy(), y=movies_merged2.iloc[:,2].to_numpy(), z=movies_merged2.iloc[:,movies_merged.shape[1]-2].to_numpy(),
            mode='markers',
            marker=dict(size=5, color=classes, opacity=1)
        ),])
    fig.show();
    the_final_output=the_selected_movie_set.head(no_of_movies);
#     print(the_final_output);
    
    listt=[];
    for i in range(len(the_final_output.index)):
        the_id=the_final_output['movieId'][the_final_output.index[i]];
        given_frame=df_movies[df_movies['movieId']==the_id]
        given_movie=given_frame['title'][given_frame.index[0]];
        listt.append(given_movie);
    listt.remove(moviename);
    
    if(fl==1):
        the_silhoutee_score =silhouette_score(movies_merged2,classes) 

        print("The Silhoutee Score for K-means  is ",the_silhoutee_score);

        from sklearn.metrics import davies_bouldin_score
        from sklearn.metrics import calinski_harabasz_score

        ## computing other type of scores. 

        s = calinski_harabasz_score(movies_merged,classes)
        DB = davies_bouldin_score(movies_merged,classes)
        print("The value of calinski_harabasz score for K-means on PCA reduced dataset is ",s);
        print("The value of davies_bouldin score for K-means on PCA reduced dataset is ",DB)
    return listt;

print(find_based_on_clustering(movies_merged,60,'Jumanji (1995)',20,1))

"""## 5. recommending movies by using Hierarichal clustering as a subpart of our algorithm"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import AgglomerativeClustering;
## as we got the best no. of clusters now we will train our final model and will get the desired results. 
def find_based_on_clustering2(data,no_of_cluster,moviename,no_of_movies,fl):
    
    model_trained=AgglomerativeClustering(n_clusters=no_of_cluster,linkage='single')
    ## training the model getting negative silhuttoe score withscalling
    
    scale=MinMaxScaler(feature_range=(0, 5));
    data2=data.copy();
    data22=data2[('rating', 'count')]
    data22=pd.DataFrame(data22, columns=[('rating', 'count')]);
    data2=data2.drop(columns=[('rating', 'count')])
    data22=scale.fit_transform(data22.to_numpy())
    data22=pd.DataFrame(data22, columns=[('rating', 'count')]);
    data22=pd.concat([data22,data2],1)   
    classes=model_trained.fit_predict(data22);
    
    ## adding a now column of class in which we are storing that a given movie belonging to which cluster. 
    movies_merged2=data22.copy();
    movies_merged2['classes']=(np.array(classes)).transpose(); 
        
    given_frame=df_movies[df_movies['title']==moviename]
    given_id=given_frame['movieId'][given_frame.index[0]];
    
    the_frame=movies_merged2[movies_merged2['movieId']==given_id];
    the_class=the_frame['classes'][the_frame.index[0]];
    
    the_selected_movie_set=movies_merged2[movies_merged2['classes']==the_class];
    the_selcted_movie_set=the_selected_movie_set.sort_values(by = ['rating','released'],ascending = False)
    
    the_final_output=the_selected_movie_set.head(no_of_movies);
#     print(the_final_output);
    
    listt=[];
    for i in range(len(the_final_output.index)):
        the_id=the_final_output['movieId'][the_final_output.index[i]];
        given_frame=df_movies[df_movies['movieId']==the_id]
        given_movie=given_frame['title'][given_frame.index[0]];
        listt.append(given_movie);
    listt.remove(moviename);
    
    if(fl==1):
        the_silhoutee_score =silhouette_score(movies_merged2,classes) 

        print("The Silhoutee Score for K-means  is ",the_silhoutee_score);

        from sklearn.metrics import davies_bouldin_score
        from sklearn.metrics import calinski_harabasz_score

        ## computing other type of scores. 

        s = calinski_harabasz_score(movies_merged,classes)
        DB = davies_bouldin_score(movies_merged,classes)
        print("The value of calinski_harabasz score for K-means on PCA reduced dataset is ",s);
        print("The value of davies_bouldin score for K-means on PCA reduced dataset is ",DB)
    return listt
print(find_based_on_clustering2(movies_merged,60,'Jumanji (1995)',20,1))

"""## 6. collaborative filtering

## 6a (based on clustering)
"""

def collaborative(userid,dataset):
    newdataset=dataset.pivot_table(index=['userId'],columns=['movieId'],values=['rating']);
    new_user_data=newdataset.loc[newdataset.index==userid];
    
    listt=new_user_data.isnull().sum();
    total_movies=[];
    for j in new_user_data.columns:
        if(new_user_data[j].isnull().values.any()==False):
            total_movies.append(j);
            
    df_new=newdataset[total_movies[0]]
    for i in range(1,len(total_movies)):
        df_neww=newdataset[total_movies[i]];
        df_new=pd.concat([df_new,df_neww],axis=1);
    
    ## got the new dataframe that are having coloumn of only the movies watched by given user.
    user_count = df_new.T.notnull().sum()
    user_count = user_count.reset_index()
    user_count.columns = ["userId", "movie_count"]
    user_count=user_count.sort_values(by=["movie_count"],ascending=False)

    the_related_user=user_count['userId'][user_count.index[1]];
    
    the_dataset=user_merged[user_merged['userId']==the_related_user]
    
    the_dataset=the_dataset.sort_values(by=['rating','released'],ascending=False)
    
    so_the_best_movie=the_dataset['title'][the_dataset.index[0]];
#     movie_based(user_merged,100,so_the_best_movie)
    list1 = find_based_on_clustering2(movies_merged,60,'Jumanji (1995)',100,0)
    list2 = find_based_on_clustering2(movies_merged,60,'Jumanji (1995)',100,0)
    
## we have the movies recommended by both models.
    user_movies=user_merged[user_merged['userId']==11846]['title'].unique()
    newlistt=[];
    for j in list2:
        if(j in list1 and j not in user_movies):
            newlistt.append(j);
    return newlistt[0:15];
   
    
print(collaborative(11846,user_merged))

"""## colaborative filtering (second type). """

def collaborative2(userid,dataset):
    ## making the pivot table
    newdataset=dataset.pivot_table(index=['userId'],columns=['movieId'],values=['rating']);
    new_user_data=newdataset.loc[newdataset.index==userid];
    
    ## finding the total movies watched by given user. 
    listt=new_user_data.isnull().sum();
    total_movies=[];
    for j in new_user_data.columns:
        if(new_user_data[j].isnull().values.any()==False):
            total_movies.append(j);
     
    ## finding the new dataset that has only the movies wathced by user as column .
    df_new=newdataset[total_movies[0]]
    for i in range(1,len(total_movies)):
        df_neww=newdataset[total_movies[i]];
        df_new=pd.concat([df_new,df_neww],axis=1);
    
    ## got the new dataframe that are having coloumn of only the movies watched by given user.
    user_count = df_new.T.notnull().sum()
    user_count = user_count.reset_index()
    user_count.columns = ["userId", "movie_count"]
    user_count=user_count.sort_values(by=["movie_count"],ascending=False)

    the_related_user=user_count['userId'][user_count.index[1]];
    
    the_dataset=user_merged[user_merged['userId']==the_related_user]
    
    the_dataset=the_dataset.sort_values(by=['rating','released'],ascending=False)
    
    so_the_best_movie=the_dataset['title'][the_dataset.index[0]];
    movie_based(user_merged,10,so_the_best_movie)

collaborative2(11846,user_merged)

duplicate_movies=df_movies[df_movies.duplicated(subset='title',keep=False)]
duplicate_movies

df_tags2=df_tags.drop(['year','month'],axis=1)
user_merged_temp=pd.merge(df_tags2,df_ratings, on=["userId",'movieId']);
user_merged=pd.merge(user_merged_temp,df_movies, on=['movieId']);
user_merged

"""# **Other Supervised Models**

## **XGBoost Model**
Predictiong new movie based on information about the movie like Genre, Released_Year_Band, Tag, Times_Watched, Rating,User_data
"""

user_merged2=user_merged;
print("NULL values in the dataset:\n",user_merged2.isnull().sum());
user_merged2=user_merged2.dropna()
user_merged2['released'] = pd.to_numeric(user_merged2['released']);
user_merged2['released'].replace({0:'2000','nan':'2000'}, inplace=True)

# print(user_merged2['released'].min());
# print(user_merged2['released'].max());
# print(user_merged['released'].unique())

# Code for Binning
# user_merged.loc[user_merged['released'].between(1892, 1950, 'both'), 'released_interval'] = '1892-1950'
# user_merged.loc[user_merged['released'].between(1950, 1990, 'right'), 'released_interval'] = '1950-1990'
# user_merged.loc[user_merged['released'].between(1990, 2005, 'right'),'released_interval'] = '1990-2005'

user_merged2.drop(columns=['title','userId'],inplace=True);
user_mergedx=user_merged2.drop(columns=['movieId']);
user_mergedy=user_merged2['movieId'];

xuser_train,xuser_test,yuser_train,yuser_test=train_test_split(user_mergedx.iloc[:50000,:],user_mergedy.iloc[:50000],test_size=0.20);
xuser_train

user_merged2['year'] = pd.to_numeric(user_merged2['year']);
user_merged2['month'] = pd.to_numeric(user_merged2['month']);

cols_to_scale=['rating','year','month','released'];
standard_scaler=StandardScaler();
standard_scaler.fit(xuser_train)
xuser_train2=standard_scaler.fit_transform(xuser_train);
xuser_test2=standard_scaler.fit_transform(xuser_test);
xuser_train2

le=LabelEncoder();
y_values = le.fit_transform(yuser_train);

XGB_model=XGBClassifier();
XGB_model.fit(xuser_train2,y_values);

def XGB_classifier(XGB_model,xuser_test2,df_movie_Id,xuser_test):
    yuser_pred=XGB_model.predict(xuser_test2);
    yuser_pred_df=pd.DataFrame(yuser_pred,columns=['movieId']);
    xuser_test['movieId']=yuser_pred;
    user_merged1=pd.merge(xuser_test,df_movie_Id, on=['movieId']);
    
    return user_merged1

XGB_classifier(XGB_model,xuser_test2,df_movie_Id,xuser_test)

"""# **Decision Tree Model**
Working on User and movie dataset with time of watching to get new movie prediction
"""

df_user_ratings
df_user_movie=df_user_ratings.drop(columns=['year','month','rating']);
user_merged3=pd.merge(df_updated_mb,df_user_movie, on=['movieId']);
df_movies_up=df_movies.loc[:,['movieId','released']];
user_merged4=pd.merge(df_movies_up,user_merged3, on=['movieId']);
df_movies_up

print("NULL values in the dataset:\n",user_merged4.isnull().sum());
user_merged4=user_merged4.dropna()

def Decision_tree_classifer(user_merged4,df_movie_Id,user_input):
    #taking movies which are already watched by the user (not to recommand)
    data_temp=user_merged4[user_merged4['userId']==user_input];
    movie_row=np.array(data_temp['movieId']);
    rows,cols=user_merged4.shape;
    data_x=data_temp.drop(columns=['movieId']);
    data_x['rating']=5;
    
    # k decision Tree Classifiers
    recommand_movies=[];
    for j in range(0,5):
        dt_classifier=DecisionTreeClassifier();
        user_merged5=user_merged4.sample(n=50000);
        user_y=user_merged5['movieId'];
        user_x=user_merged5.drop(columns=['movieId']);
        
        #Fitting the model
        print("reached here!!!")
        dt_classifier.fit(user_x,user_y);
        predict_val=dt_classifier.predict(data_x);
#         print(predict_val)
        r_movies=[];
        for i in range(0,len(predict_val)):
            if predict_val[i] not in recommand_movies:
                if predict_val[i] not in movie_row:
                    recommand_movies.append(predict_val[i]);
#     out_movies=[];
#     for i in range(0,len(recommand_movies)):
#         if(recommand_movies.count(recommand_movies[i])>1):
#             if recommand_movies[i] not in out_movies:
#                 out_movies.append(recommand_movies[i])
    recommand_df=pd.DataFrame(recommand_movies,columns=['movieId']);
    recommand_merged=pd.merge(recommand_df,df_movie_Id, on=['movieId']);
    return recommand_merged;
    

user_merged41=user_merged4.drop(columns=[(  'rating', 'count')]);
user_merged41['rating_count']=user_merged4[(  'rating', 'count')]; 
Decision_tree_classifer(user_merged41,df_movie_Id,11846)

Decision_tree_classifer(user_merged41,df_movie_Id,2)

"""# **Neural Network Based Model**"""

import tensorflow.compat.v1 as tf
tf.compat.v1.disable_eager_execution()
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

# df_movie_Id['index'] = df_movie_Id.index
# df_movie_Id2=df_movie_Id.iloc[:100,:]
# df_movie_Id2
movies_merg=movies_merged.loc[:,['movieId','rating']];
movies_merg['count']=movies_merged[('rating', 'count')];
movies_mergy=movies_merg['movieId'];
movies_mergx=movies_merg.drop(columns=['movieId']);
mm_scaler=MinMaxScaler();
mm_scaler.fit(movies_mergx);
movies_mergx2=mm_scaler.fit_transform(movies_mergx);

movies_mergx2=pd.DataFrame(movies_mergx2,columns=['rating','count']);
# pca=PCA(n_components=1);
# pca.fit(movies_mergx2);
# movies_mergx3=pca.transform(movies_mergx2);
# movies_merg3=pd.DataFrame(movies_mergx3,columns=['rating']);
movies_mergx2.loc[:,'rating'] = movies_mergx2.loc[:,'rating'].add(movies_mergx2.loc[:,'count'])
# movies_mergx2['rating']=movies_mergx2['rating2']+movies_mergx2['count']/2;
movies_mergx2['movieId']=movies_mergy;
movies_mergx2=movies_merg.drop(columns=['count']);
movies_mergx2['index'] = movies_mergx2.index
df_movie_Id2=movies_mergx2.iloc[:1000,:]
df_movie_Id2

df_ratings3=df_ratings2.drop(columns=['rating']);
merged_data = df_movie_Id2.merge(df_ratings3.iloc[:3500000,:], on='movieId');
merged_data.drop(columns=['year','month'],inplace=True);
user_grouped = merged_data.groupby('userId');
user_grouped.head()

"""transforming the data for input to RBM"""

print(merged_data['index'].unique());
print(len(merged_data['movieId'].unique()));
train_list=[];
len_moviedata2=len(df_movie_Id)
len_moviedata=len(merged_data['movieId'].unique());
# number_of_users=1000;
for userId,curr_user in user_grouped:
    #Storing rating of every movie
    store=[0]*len_moviedata;
    for num ,movie in curr_user.iterrows():
#         print(num);
#         print(movie);
#         print(movie['index'])
        store[int(movie['index'])] = movie['rating']/4.0
    train_list.append(store);
# print(train_list)

number_of_hl=10;
number_of_vl=len_moviedata;
vb = tf.placeholder("float",[number_of_vl]) #biases of visible layer
hb = tf.placeholder("float",[number_of_hl]) #handling biases of hidden layer
W = tf.placeholder("float",[number_of_vl, number_of_hl])

#setting activation functions relu and sigmoid
v0 = tf.placeholder("float", [None, number_of_vl])# tensorflow placeholder for input data, representing input layer
_h0= tf.nn.sigmoid(tf.matmul(v0,W) + hb)# calculates the activation of hidden layer
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))#computes the hidden layer state using the activation

_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) #computes the activation of visible layer given the state of hidden layer
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))#input layer state calc. using activation data
h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)#activation of hidden layer given the state of visible layer

# setting RBM parameters, functions and the error functions
error=(v0-v1);
error_sum = tf.reduce_mean(error*error);
# setting the learning rate
alpha=0.01;
#to create the gradint of W and to calculate the constrastive divergence
wpos_grad = tf.matmul(tf.transpose(v0), h0);
wneg_grad = tf.matmul(tf.transpose(v1), h1);
Contra_D = (wpos_grad-wneg_grad) / tf.to_float(tf.shape(v0)[0]);

# Creating method to update weigths and biases
w_update = W + alpha * Contra_D
vb_update = vb + alpha*tf.reduce_mean(v0-v1,0)
hb_update = hb + alpha*tf.reduce_mean(h0-h1,0)

#Initializing our variables
current_w=np.zeros([number_of_vl,number_of_hl],np.float32);
current_vb=np.zeros([number_of_vl],np.float32);
current_hb=np.zeros([number_of_hl],np.float32);
previous_w=np.zeros([number_of_vl,number_of_hl],np.float32);
previous_vb=np.zeros([number_of_vl],np.float32);
previous_hb=np.zeros([number_of_hl],np.float32);
session=tf.compat.v1.Session();
session.run(tf.compat.v1.global_variables_initializer());

num_epochs=10;
batch_size=100;
for i in range(batch_size,len(train_list),batch_size):
    start=i-batch_size;
    end=i;
    batch=train_list[start:end];
    current_w = session.run(w_update, feed_dict={v0: batch, W: previous_w, vb: previous_vb, hb: previous_hb})
    current_vb = session.run(vb_update, feed_dict={v0: batch, W: previous_w, vb: previous_vb, hb: previous_hb})
    current_hb = session.run(hb_update, feed_dict={v0: batch, W: previous_w, vb: previous_vb, hb: previous_hb})
    previous_w=current_w;
    previous_vb=current_vb;
    previous_hb=current_hb;

hh0=tf.nn.sigmoid(tf.matmul(v0,W)+hb);
user_to_be_recommanded=[train_list[5]];
feed = session.run(tf.nn.sigmoid(tf.matmul(v0,W)+hb),feed_dict={ v0: user_to_be_recommanded, W: previous_w, hb: previous_hb})
recommandation_score = session.run(tf.nn.sigmoid(tf.matmul(hh0,tf.transpose(W))+vb), feed_dict={ hh0: feed, W: previous_w, vb: previous_vb})

df_movie_Id2['r_score']=recommandation_score[0];
df_output=df_movie_Id2.merge(df_movie_Id, on='movieId');
df_output.sort_values(["r_score"],ascending=False);

df_output

"""#Implementing models using Surprise Library"""

import os
from surprise import Reader
from surprise import Dataset

# modify the file path to point to the Kaggle dataset directory
file_path = os.path.expanduser('/kaggle/input/movie-recommendation-system/ratings.csv')
df = pd.read_csv(file_path)
df=df.sample(n=10000)
# use the Reader class to parse the file
reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale=(1.00,5.00), skip_lines=1)

# load the dataset from the file using the reader
data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader=reader)

from surprise import AlgoBase
from surprise import SVD
from collections import defaultdict
from surprise.model_selection import cross_validate

class OwnSurprise():
    def __init__(self, algorithm):
        self.algorithm = algorithm
        self.predictions = []
        self.top_pred = defaultdict(list)
        self.item_ids=[]
    
    def fit(self, train, test):
        self.algorithm.fit(train)
        self.predictions = self.algorithm.test(test)
    
    def top_predict(self, n=5):
        self.n = n
        for userid, innerid, true_rating, est_rating, _ in self.predictions:
            self.top_pred[userid].append((innerid, est_rating))                       
        for userid, user_ratings in self.top_pred.items():
            user_ratings.sort(key=lambda x: x[1], reverse=True)
            self.top_pred[userid] = user_ratings[:n]
        return self.top_pred
    def show(self):
        for userid, user_ratings in self.top_pred.items():
            print(userid, [innerid for (innerid, _) in user_ratings])
    def specific_show(self, givenid):
        if givenid in self.top_pred.keys():
            user_ratings = self.top_pred[givenid]
            self.item_ids = [innerid for (innerid, _) in user_ratings]
            
            print(f"User {givenid} rated the following items:")
            print(self.item_ids)
            return self.item_ids
        else:
            print(f"User {givenid} not found in recommendations.")
            
    def movies_recommend(self,data,index):
        self.data=data
        for i in index:
            rows = data['title'][i]
            print("The movies recommended are :",rows)
            print("\n")

from surprise import Dataset
from surprise.model_selection import train_test_split
trainset = data.build_full_trainset()
testset = trainset.build_anti_testset()
algo = SVD()
surprise=OwnSurprise(algo)
surprise.fit(trainset,testset)
pred=surprise.top_predict(n=3)

surprise.show()

index=surprise.specific_show(52575)

surprise.movies_recommend(df_movies,index)

from surprise import Dataset,KNNWithMeans
from surprise.model_selection import train_test_split
trainset = data.build_full_trainset()
testset = trainset.build_anti_testset()
sim_options = {'name': 'cosine', 'user_based': True}
algo = KNNWithMeans(sim_options=sim_options)
surprise=OwnSurprise(algo)
surprise.fit(trainset,testset)
pred=surprise.top_predict(n=3)

surprise.show()

index=surprise.specific_show(52575)

surprise.movies_recommend(df_movies,index)